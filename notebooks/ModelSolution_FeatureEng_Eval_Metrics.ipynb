{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96bf8961-fc65-4b2b-ac0f-6b1f109219f3",
   "metadata": {},
   "source": [
    "# Feature Engineering, Model, Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd6bbb-b50d-49af-8dbf-29dee2aa7c32",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "**Note:** We suppose that the `EDA.ipynb` file has already been run to remove duplicates and handle missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c89e5528-1623-46ec-8edb-96983e36e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, expr, explode, collect_list, collect_set,\n",
    "    udf, size\n",
    ")\n",
    "from pyspark.sql.types import ArrayType, IntegerType, FloatType\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "\n",
    "\n",
    "small_matrix = pd.read_csv(\"../data_final_project/KuaiRec/data/small_matrix.csv\")\n",
    "big_matrix = pd.read_csv(\"../data_final_project/KuaiRec/data/big_matrix.csv\")\n",
    "\n",
    "caption_category = pd.read_csv(\"../data_final_project/KuaiRec/data/kuairec_caption_category.csv\", engine=\"python\", sep=\",\", quotechar='\"', on_bad_lines='skip')\n",
    "\n",
    "item_categories = pd.read_csv(\"../data_final_project/KuaiRec/data/item_categories.csv\")\n",
    "\n",
    "item_daily_features = pd.read_csv(\"../data_final_project/KuaiRec/data/item_daily_features.csv\")\n",
    "item_daily_features.fillna(-1, inplace=True)\n",
    "\n",
    "user_features = pd.read_csv(\"../data_final_project/KuaiRec/data/user_features.csv\")\n",
    "user_features.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a0c310-29a7-4695-aa1e-219323d9cff8",
   "metadata": {},
   "source": [
    "##  Setting up spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05cde22b-9237-452b-963e-7b9a46a40174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/17 20:31:38 WARN Utils: Your hostname, Euginux resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/05/17 20:31:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/17 20:31:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KuaiRec ALS with Metadata\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a55281-8db7-46fe-bc98-70188ef23b63",
   "metadata": {},
   "source": [
    "##  Feature engineering\n",
    "\n",
    "We first initialize to process large-scale data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b01312-8075-4b89-a24f-66e57127cc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Load interaction data\n",
    "small_matrix = spark.read.csv(\n",
    "    \"../data_final_project/KuaiRec/data/small_matrix.csv\",\n",
    "    header=True,\n",
    "    sep=\",\",\n",
    "    nullValue=\"\",\n",
    "    inferSchema=True,\n",
    ").select(\"user_id\", \"video_id\", \"watch_ratio\").na.drop(subset=[\"user_id\", \"video_id\", \"watch_ratio\"])\n",
    "\n",
    "# Load user metadata\n",
    "user_metadata = spark.read.csv(\n",
    "    \"../data_final_project/KuaiRec/data/user_features.csv\",\n",
    "    header=True,\n",
    "    sep=\",\",\n",
    "    nullValue=\"\",\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "# Load item metadata\n",
    "item_metadata = spark.read.csv(\n",
    "    \"../data_final_project/KuaiRec/data/item_daily_features.csv\",\n",
    "    header=True,\n",
    "    sep=\",\",\n",
    "    nullValue=\"\",\n",
    "    inferSchema=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096baa29-6713-4e79-901f-384db89b0c51",
   "metadata": {},
   "source": [
    "Enrich interaction data by joining with user and item metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d61d70-e2e6-492f-bfb0-c883c6826a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_matrix_with_user_metadata = small_matrix.join(\n",
    "    user_metadata,\n",
    "    on=\"user_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "small_matrix_with_metadata = small_matrix_with_user_metadata.join(\n",
    "    item_metadata,\n",
    "    on=\"video_id\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc2829d-2523-4309-9382-3fc778ae7eb3",
   "metadata": {},
   "source": [
    "Define user-related features and cast them to numerical format for modeling and assemble into a single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347743f3-c8d1-4117-82ce-42cb5e5876cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature_columns = [\"is_lowactive_period\", \"fans_user_num\", \"register_days\"]\n",
    "\n",
    "for column in user_feature_columns:\n",
    "    small_matrix_with_metadata = small_matrix_with_metadata.withColumn(\n",
    "        column, col(column).cast(\"double\")\n",
    "    )\n",
    "\n",
    "user_assembler = VectorAssembler(\n",
    "    inputCols=user_feature_columns,\n",
    "    outputCol=\"user_features\"\n",
    ")\n",
    "small_matrix_with_metadata = user_assembler.transform(small_matrix_with_metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c12ac-7a76-4f2f-8500-0c3eb81f9130",
   "metadata": {},
   "source": [
    "\n",
    "Define item-related features and cast them to numerical format for modeling and assemble into a single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99687b4c-352c-449e-a2a9-771da1e513bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_columns = [\"video_duration\", \"play_cnt\", \"complete_play_user_num\", \n",
    "                       \"like_user_num\", \"comment_cnt\", \"share_user_num\"]\n",
    "\n",
    "for column in item_feature_columns:\n",
    "    small_matrix_with_metadata = small_matrix_with_metadata.withColumn(\n",
    "        column, col(column).cast(\"double\")\n",
    "    )\n",
    "\n",
    "item_assembler = VectorAssembler(\n",
    "    inputCols=item_feature_columns,\n",
    "    outputCol=\"item_features\"\n",
    ")\n",
    "small_matrix_with_metadata = item_assembler.transform(small_matrix_with_metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e793c299-a189-442b-8e65-76331d87698b",
   "metadata": {},
   "source": [
    "##  Model architecture\n",
    "\n",
    "We configure and tune an ALS collaborative filtering model using cross-validation with different hyperparameters. The data is split into training and test sets, and the model is trained and evaluated using RMSE. Finally, we save the best-performing model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a136b7a-5a06-469b-90d8-86c4fa86663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.286511979447459\n",
      "Rank: 10\n",
      "MaxIter: 15\n",
      "RegParam: 0.1\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3335.save.\n: java.io.IOException: Path als_best_model already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaxIter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpyspark_als_model\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mparent()\u001b[38;5;241m.\u001b[39mgetMaxIter()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegParam: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpyspark_als_model\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mparent()\u001b[38;5;241m.\u001b[39mgetRegParam()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mpyspark_als_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mals_best_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/util.py:262\u001b[0m, in \u001b[0;36mMLWritable.save\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/util.py:213\u001b[0m, in \u001b[0;36mJavaMLWriter.save\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3335.save.\n: java.io.IOException: Path als_best_model already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "als = ALS(\n",
    "    maxIter=10,\n",
    "    rank=10,\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"video_id\",\n",
    "    ratingCol=\"watch_ratio\",\n",
    "    implicitPrefs=True\n",
    ")\n",
    "\n",
    "params = ParamGridBuilder() \\\n",
    "    .addGrid(als.maxIter, [10, 15]) \\\n",
    "    .addGrid(als.regParam, [0.1]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"watch_ratio\", predictionCol=\"prediction\")\n",
    "\n",
    "cvs = CrossValidator(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=params,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    ")\n",
    "\n",
    "# we use small_matrix instead of big_matrix and split it into train and test\n",
    "# Spark is not able to support a data set as big as big_matrix and crashes\n",
    "(training, test) = small_matrix.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "models = cvs.fit(training)\n",
    "\n",
    "# Take the best model from the CrossValidator\n",
    "pyspark_als_model = models.bestModel\n",
    "predictions = pyspark_als_model.transform(test)\n",
    "rmse = evaluator.evaluate(predictions.na.drop())\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Rank: {pyspark_als_model.rank}\")\n",
    "print(f\"MaxIter: {pyspark_als_model._java_obj.parent().getMaxIter()}\")\n",
    "print(f\"RegParam: {pyspark_als_model._java_obj.parent().getRegParam()}\")\n",
    "\n",
    "pyspark_als_model.save(\"als_best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cc495d-8ac6-417f-8021-05bb7e7c8c7e",
   "metadata": {},
   "source": [
    "# Recommendation\n",
    "\n",
    "We have the ALS model with its hyperparameters. We now need to build the recommendation function. \n",
    "We have to: \n",
    "For each user, predict ratings/scores for candidate videos they haven’t watched.\n",
    "\n",
    "Rank these videos by predicted rating.\n",
    "\n",
    "Return top N videos per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f04d49-d3f4-48db-aa22-ef60fde31712",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "\n",
    "test_users = test.select(\"user_id\").distinct()\n",
    "\n",
    "user_recs = pyspark_als_model.recommendForUserSubset(test_users, top_n)\n",
    "\n",
    "user_recs_exploded = user_recs.select(\n",
    "    col(\"user_id\"),\n",
    "    explode(\"recommendations\").alias(\"rec\")\n",
    ").select(\n",
    "    col(\"user_id\"),\n",
    "    col(\"rec.video_id\"),\n",
    "    col(\"rec.rating\")\n",
    ")\n",
    "\n",
    "user_recs_exploded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34244420-f44f-40ed-bfd2-ee703defbf9f",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeaab79f-7617-4a67-ad00-eeb782dc9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only watch_ratio > 0 are relevant\n",
    "test_relevant = test.filter(col(\"watch_ratio\") > 0)\n",
    "\n",
    "ground_truth = test_relevant.groupBy(\"user_id\").agg(\n",
    "    collect_set(\"video_id\").alias(\"true_items\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5d7a40d-5934-43cd-ac3f-54d687034991",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "test_users = test.select(\"user_id\").distinct()\n",
    "\n",
    "user_recs = pyspark_als_model.recommendForUserSubset(test_users, top_n)\n",
    "\n",
    "user_recs = user_recs.select(\n",
    "    \"user_id\",\n",
    "    expr(\"transform(recommendations, x -> x.video_id)\").alias(\"pred_items\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32a11c1f-5175-48e3-be44-e940505dfed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = user_recs.join(ground_truth, on=\"user_id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08021a12-12d5-43f9-934d-711afb272bb5",
   "metadata": {},
   "source": [
    "# Defining metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdb9673a-7cda-45c4-b4d8-816130e1672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(pred_items, true_items, k=10):\n",
    "    pred_k = pred_items[:k]\n",
    "    hits = len(set(pred_k) & set(true_items))\n",
    "    return hits / float(k)\n",
    "\n",
    "def recall_at_k(pred_items, true_items, k=10):\n",
    "    pred_k = pred_items[:k]\n",
    "    hits = len(set(pred_k) & set(true_items))\n",
    "    return hits / float(len(true_items)) if true_items else 0.0\n",
    "\n",
    "precision_udf = udf(lambda pred, true: precision_at_k(pred, true, top_n), FloatType())\n",
    "recall_udf = udf(lambda pred, true: recall_at_k(pred, true, top_n), FloatType())\n",
    "\n",
    "eval_df = eval_df.withColumn(\"precision_at_k\", precision_udf(\"pred_items\", \"true_items\"))\n",
    "eval_df = eval_df.withColumn(\"recall_at_k\", recall_udf(\"pred_items\", \"true_items\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a46441a-0e55-467f-9701-ac9854e1fa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision@10: 0.1928\n",
      "Mean Recall@10: 0.0030\n"
     ]
    }
   ],
   "source": [
    "avg_precision = eval_df.selectExpr(\"avg(precision_at_k) as mean_precision\").collect()[0][\"mean_precision\"]\n",
    "avg_recall = eval_df.selectExpr(\"avg(recall_at_k) as mean_recall\").collect()[0][\"mean_recall\"]\n",
    "\n",
    "print(f\"Mean Precision@{top_n}: {avg_precision:.4f}\")\n",
    "print(f\"Mean Recall@{top_n}: {avg_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ab003-067d-4859-8b34-11c19acb7eaa",
   "metadata": {},
   "source": [
    "### Average Precision and NDCG@K\n",
    "\n",
    "MAP (Mean Average Precision): Average precision across all users, accounting for order of hits.\n",
    "\n",
    "NDCG: Measures usefulness considering position; higher rank hits contribute more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "185a06ad-361a-45c6-8d7f-891b895a38cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(pred_items, true_items, k=10):\n",
    "    if not true_items:\n",
    "        return 0.0\n",
    "    pred_items = pred_items[:k]\n",
    "    score = 0.0\n",
    "    hits = 0\n",
    "    for i, item in enumerate(pred_items):\n",
    "        if item in true_items:\n",
    "            hits += 1\n",
    "            score += hits / (i + 1.0)\n",
    "    return score / min(len(true_items), k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e598fd4-7b7c-4514-b80c-dca1fed5d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def ndcg(pred_items, true_items, k=10):\n",
    "    pred_items = pred_items[:k]\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(pred_items):\n",
    "        if item in true_items:\n",
    "            dcg += 1.0 / math.log2(i + 2)  # +2 because log2(1) for position 0\n",
    "    ideal_dcg = sum([1.0 / math.log2(i + 2) for i in range(min(len(true_items), k))])\n",
    "    return dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85e89913-40b2-4d7f-acf5-77e8173b6c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@10: 0.0827\n",
      "NDCG@10: 0.1930\n"
     ]
    }
   ],
   "source": [
    "ap_udf = udf(lambda pred, true: average_precision(pred, true, top_n), FloatType())\n",
    "ndcg_udf = udf(lambda pred, true: ndcg(pred, true, top_n), FloatType())\n",
    "\n",
    "# we apply udf and eval\n",
    "eval_df = eval_df \\\n",
    "    .withColumn(\"average_precision\", ap_udf(\"pred_items\", \"true_items\")) \\\n",
    "    .withColumn(\"ndcg\", ndcg_udf(\"pred_items\", \"true_items\"))\n",
    "\n",
    "mean_ap = eval_df.selectExpr(\"avg(average_precision) as MAP\").first()[\"MAP\"]\n",
    "mean_ndcg = eval_df.selectExpr(\"avg(ndcg) as NDCG\").first()[\"NDCG\"]\n",
    "\n",
    "print(f\"MAP@{top_n}: {mean_ap:.4f}\")\n",
    "print(f\"NDCG@{top_n}: {mean_ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83f50d-878d-427b-81ff-e1169624a5e9",
   "metadata": {},
   "source": [
    "## Evaluation on a random basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1a6383b-4e77-49bf-90cb-cd3c938bc773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Precision@10: 0.0536\n",
      "Random Recall@10: 0.0008\n",
      "Random MAP@10: 0.0161\n",
      "Random NDCG@10: 0.0512\n"
     ]
    }
   ],
   "source": [
    "big_matrix_spark = spark.read.csv(\n",
    "    \"../data_final_project/KuaiRec/data/big_matrix.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").select(\"video_id\").na.drop()\n",
    "\n",
    "\n",
    "all_videos = small_matrix.select(\"video_id\").union(\n",
    "    big_matrix_spark.select(\"video_id\")\n",
    ").distinct()\n",
    "video_list = [row.video_id for row in all_videos.collect()]\n",
    "\n",
    "def random_recommendations(video_pool, k=10):\n",
    "    return random.sample(video_pool, k)\n",
    "\n",
    "random_rec_udf = udf(lambda: random.sample(video_list, 10), ArrayType(IntegerType()))\n",
    "\n",
    "random_preds = test.select(\"user_id\").distinct().withColumn(\"pred_items\", random_rec_udf())\n",
    "\n",
    "random_eval_df = random_preds.join(ground_truth, on=\"user_id\")\n",
    "\n",
    "random_eval_df = random_preds.join(ground_truth, on=\"user_id\")\n",
    "\n",
    "random_eval_df = random_eval_df \\\n",
    "    .withColumn(\"precision_at_k\", precision_udf(\"pred_items\", \"true_items\")) \\\n",
    "    .withColumn(\"recall_at_k\", recall_udf(\"pred_items\", \"true_items\")) \\\n",
    "    .withColumn(\"average_precision\", ap_udf(\"pred_items\", \"true_items\")) \\\n",
    "    .withColumn(\"ndcg\", ndcg_udf(\"pred_items\", \"true_items\"))\n",
    "\n",
    "random_metrics = random_eval_df.selectExpr(\n",
    "    \"avg(precision_at_k) as precision\",\n",
    "    \"avg(recall_at_k) as recall\",\n",
    "    \"avg(average_precision) as map\",\n",
    "    \"avg(ndcg) as ndcg\"\n",
    ").first()\n",
    "\n",
    "print(f\"Random Precision@10: {random_metrics['precision']:.4f}\")\n",
    "print(f\"Random Recall@10: {random_metrics['recall']:.4f}\")\n",
    "print(f\"Random MAP@10: {random_metrics['map']:.4f}\")\n",
    "print(f\"Random NDCG@10: {random_metrics['ndcg']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e7314-66d5-4886-a3b2-c8a149aabe90",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Evaluation metrics on Top-10 with ALS Model:\n",
    "- **Mean Precision@10:** `0.1904`\n",
    "- **Mean Recall@10:** `0.0029`\n",
    "- **MAP@10 (Mean Average Precision):** `0.0797`\n",
    "- **NDCG@10 (Normalized Discounted Cumulative Gain):** `0.1876`\n",
    "\n",
    "---\n",
    "\n",
    "### Random Baseline (for comparing):\n",
    "- **Random Precision@10:** `0.0623`\n",
    "- **Random Recall@10:** `0.0009`\n",
    "- **Random MAP@10:** `0.0209`\n",
    "- **Random NDCG@10:** `0.0624`\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- **Precision@10 (19.04%)**: On average, ~2 of the top 10 recommended videos are relevant — significantly better than random (6.23%).  \n",
    "- **Recall@10 (0.29%)**: The model retrieves only a very small portion of all relevant items. This may reflect a large candidate set and limited top-K budget.\n",
    "- **MAP@10 (7.97%)**: Relevant videos appear earlier in the list more often than random, but there's still a lot of room for ranking improvement.\n",
    "- **NDCG@10 (18.76%)**: Shows decent ranking quality — relevant items are typically placed higher than in a random list.\n",
    "\n",
    "Compared to the **random recommender**, the ALS model performs **substantially better** across all metrics, especially in **precision and ranking (MAP/NDCG)**.\n",
    "\n",
    "### Potential improvements or ideas:\n",
    "\n",
    "- Tuning more hyperparameter tuning (`rank`, `regParam`, `alpha`).\n",
    "- Address cold-start issues (e.g., filter unseen users/items or integrate side info).\n",
    "- Look into hybrid models combining collaborative filtering with metadata.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
